{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/19 16:57:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Get the data from the parquet file\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# memory issues .. see https://stackoverflow.com/questions/21138751/spark-java-lang-outofmemoryerror-java-heap-space\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", True).config(\"spark.memory.offHeap.size\",\"8g\") \\\n",
    "    .appName(\"PropertyData\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
    "spark\n",
    "df = spark.read.parquet(\"/tmp/spark/tarrant-tx.parquet\")\n",
    "\n",
    "# Property used to format output tables better\n",
    "# spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T21:57:11.129119626Z",
     "start_time": "2023-06-19T21:57:05.054804543Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/19 16:57:14 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "23/06/19 17:03:10 WARN DAGScheduler: Broadcasting large task binary with size 22.8 MiB\n",
      "[Stage 38:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "subset = [\n",
    "    'Total_Value', 'Appraised_Value_Int', 'Central_Heat_Ind_idx', 'Central_Air_Ind_idx', 'Swimming_Pool_Ind_idx'\n",
    "]\n",
    "\n",
    "# large DF causes \"java.lang.NegativeArraySizeException\"\n",
    "logistic_df = df.select(*subset).limit(1000)\n",
    "\n",
    "logistic_features = [\n",
    "    'Total_Value', 'Central_Heat_Ind_idx', 'Central_Air_Ind_idx', 'Swimming_Pool_Ind_idx',\n",
    "]\n",
    "\n",
    "model_logistic = LogisticRegression(featuresCol=\"features\", labelCol=\"Appraised_Value_Int\", maxIter=5)\n",
    "\n",
    "# Train\n",
    "train_logistic, test_logistic = logistic_df.randomSplit([0.7, 0.3])\n",
    "assembler = VectorAssembler(inputCols=logistic_features, outputCol='features')\n",
    "train_logistic = assembler.transform(train_logistic)\n",
    "test_logistic = assembler.transform(test_logistic)\n",
    "logistic_trained_model = model_logistic.fit(train_logistic)\n",
    "\n",
    "# Evaluate\n",
    "logistic_predictions = logistic_trained_model.transform(test_logistic)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='Appraised_Value_Int', rawPredictionCol='prediction')\n",
    "accuracy = evaluator.evaluate(logistic_predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T22:03:12.524078692Z",
     "start_time": "2023-06-19T21:57:12.184956895Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/19 17:09:06 WARN DAGScheduler: Broadcasting large task binary with size 22.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+--------------------+-------------------+---------------------+-----------------+--------------------+--------------------+----------+\n",
      "|Total_Value|Appraised_Value_Int|Central_Heat_Ind_idx|Central_Air_Ind_idx|Swimming_Pool_Ind_idx|         features|       rawPrediction|         probability|prediction|\n",
      "+-----------+-------------------+--------------------+-------------------+---------------------+-----------------+--------------------+--------------------+----------+\n",
      "|        1.0|                  1|                 0.0|                0.0|                  0.0|    (4,[0],[1.0])|[-0.0016698163476...|[8.14378380962082...|  161880.0|\n",
      "|        1.0|                  1|                 0.0|                0.0|                  1.0|[1.0,0.0,0.0,1.0]|[-0.0016302136903...|[1.42234180864383...|  271586.0|\n",
      "|    15681.0|              15681|                 0.0|                0.0|                  0.0|(4,[0],[15681.0])|[-0.0016735901750...|[1.17742240593027...|  161880.0|\n",
      "|    16800.0|              16800|                 0.0|                0.0|                  0.0|(4,[0],[16800.0])|[-0.0016738594934...|[1.20830579811214...|  161880.0|\n",
      "|    17550.0|              17550|                 0.0|                0.0|                  0.0|(4,[0],[17550.0])|[-0.0016740400017...|[1.22941933797700...|  161880.0|\n",
      "|    18000.0|              18000|                 0.0|                0.0|                  0.0|(4,[0],[18000.0])|[-0.0016741483067...|[1.24224940281558...|  161880.0|\n",
      "|    21000.0|              21000|                 0.0|                0.0|                  0.0|(4,[0],[21000.0])|[-0.0016748703400...|[1.33097121050633...|  161880.0|\n",
      "|    22500.0|              22500|                 0.0|                0.0|                  0.0|(4,[0],[22500.0])|[-0.0016752313566...|[1.37747751617431...|  161880.0|\n",
      "|    22500.0|              22500|                 0.0|                0.0|                  0.0|(4,[0],[22500.0])|[-0.0016752313566...|[1.37747751617431...|  161880.0|\n",
      "|    22680.0|              22680|                 0.0|                0.0|                  0.0|(4,[0],[22680.0])|[-0.0016752746786...|[1.38315725242682...|  161880.0|\n",
      "|    22779.0|              22779|                 0.0|                0.0|                  0.0|(4,[0],[22779.0])|[-0.0016752985057...|[1.38629024970308...|  161880.0|\n",
      "|    23700.0|              23700|                 0.0|                0.0|                  0.0|(4,[0],[23700.0])|[-0.0016755201700...|[1.41574990638577...|  161880.0|\n",
      "|    23700.0|              23700|                 0.0|                0.0|                  0.0|(4,[0],[23700.0])|[-0.0016755201700...|[1.41574990638577...|  161880.0|\n",
      "|    24750.0|              24750|                 0.0|                0.0|                  0.0|(4,[0],[24750.0])|[-0.0016757728816...|[1.45003503844811...|  161880.0|\n",
      "|    26250.0|              26250|                 0.0|                0.0|                  0.0|(4,[0],[26250.0])|[-0.0016761338983...|[1.50033446756956...|  161880.0|\n",
      "|    26880.0|              26880|                 0.0|                0.0|                  0.0|(4,[0],[26880.0])|[-0.0016762855253...|[1.52193282368767...|  161880.0|\n",
      "|    32000.0|              32000|                 0.0|                0.0|                  0.0|(4,[0],[32000.0])|[-0.0016775177954...|[1.70831027600652...|  161880.0|\n",
      "|    32600.0|              32600|                 0.0|                0.0|                  0.0|(4,[0],[32600.0])|[-0.0016776622021...|[1.73146652514257...|  161880.0|\n",
      "|    47225.0|              47225|                 0.0|                0.0|                  0.0|(4,[0],[47225.0])|[-0.0016811821144...|[2.39263608105969...|  161880.0|\n",
      "|    58498.0|              58498|                 0.0|                0.0|                  0.0|(4,[0],[58498.0])|[-0.0016838952748...|[3.05107560185029...|  161880.0|\n",
      "+-----------+-------------------+--------------------+-------------------+---------------------+-----------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_predictions.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T22:09:32.607978430Z",
     "start_time": "2023-06-19T22:09:06.141868732Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
