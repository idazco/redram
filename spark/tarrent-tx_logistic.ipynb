{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the data from the parquet file\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# memory issues .. see https://stackoverflow.com/questions/21138751/spark-java-lang-outofmemoryerror-java-heap-space\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", True).config(\"spark.memory.offHeap.size\",\"8g\") \\\n",
    "    .appName(\"PropertyData\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
    "spark\n",
    "df = spark.read.parquet(\"/tmp/spark/tarrant-tx.parquet\")\n",
    "\n",
    "# Property used to format output tables better\n",
    "# spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "subset = [\n",
    "    'Account_Num',  # <-- used later for analysis correlating to the original dataframe\n",
    "    'Total_Value', 'Appraised_Value_Int', 'Swimming_Pool_Ind_bin'\n",
    "]\n",
    "\n",
    "# large DF causes \"java.lang.NegativeArraySizeException\"\n",
    "logistic_df = df.select(*subset)\n",
    "# and keep only those rows that are residential properties in a certain value range\n",
    "logistic_df = logistic_df.where(\"(Total_Value between 100000 and 500000) and (RP = 'R')\").limit(100)\n",
    "\n",
    "logistic_features = ['Total_Value', 'Swimming_Pool_Ind_bin']\n",
    "\n",
    "model_logistic = LogisticRegression(featuresCol=\"features\", labelCol=\"Appraised_Value_Int\", maxIter=2)\n",
    "\n",
    "# Train\n",
    "assembler = VectorAssembler(inputCols=logistic_features, outputCol='features')\n",
    "train_logistic, test_logistic = assembler.transform(logistic_df).randomSplit([0.5, 0.5])\n",
    "logistic_trained_model = model_logistic.fit(train_logistic)\n",
    "\n",
    "# Evaluate\n",
    "predictions = logistic_trained_model.transform(test_logistic)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='Appraised_Value_Int', rawPredictionCol='prediction')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from udfs import percent_diff_udf\n",
    "\n",
    "# cast the predictions to integer and show the difference as integer and percentage\n",
    "predictions = predictions.withColumn('prediction_int', predictions['prediction'].cast('int'))\n",
    "predictions = predictions.drop('prediction')\n",
    "predictions = predictions.withColumn('diff', col('Appraised_Value_Int') - col('prediction_int'))\n",
    "predictions = predictions.withColumn('diff_percent', percent_diff_udf(col('prediction_int'), col('Appraised_Value_Int')))\n",
    "\n",
    "# create the views\n",
    "predictions.select('Account_Num', 'prediction_int', 'diff', 'diff_percent').createOrReplaceTempView(\"predictions\")\n",
    "df.createOrReplaceTempView(\"data\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# let's take a look at some of the data\n",
    "# we group the diff_percent values so there aren't too many records to graph\n",
    "# also, here we are just looking at residential properties\n",
    "\n",
    "query = \"\"\"\n",
    "with summary as (\n",
    "  with src as (\n",
    "    select FLOOR(CAST(diff_percent as INT) / 20) * 20 AS diff_group\n",
    "    from predictions\n",
    "    join data on data.Account_Num = predictions.Account_Num\n",
    "  )\n",
    "  select count(diff_group) as count, diff_group from src group by 2 order by 2 desc\n",
    ")\n",
    "select count, diff_group from summary where count > 100\n",
    "\"\"\"\n",
    "res = spark.sql(query)\n",
    "\n",
    "# graph it show how the predictions differ from the current appraisal values\n",
    "import matplotlib.pyplot as plt\n",
    "pandas_df = res.toPandas()\n",
    "plt.bar(pandas_df['diff_group'], pandas_df['count'], color='blue', width=8)\n",
    "plt.xlabel('diff_group')\n",
    "plt.ylabel('count')\n",
    "plt.title('Prediction Diff (longest line should be over \"0\")') # .. which would indicate more accurate prediction\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
