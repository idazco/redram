{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data from the website and prepare it\n",
    "\n",
    "File: PropertyData.txt  \n",
    "Source: https://www.tad.org (Tarrant Appraisal District, TX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T21:28:24.382177661Z",
     "start_time": "2023-06-19T21:28:18.428036436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing property file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import requests, zipfile, io, os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id, trim, col, when\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# memory issues .. see https://stackoverflow.com/questions/21138751/spark-java-lang-outofmemoryerror-java-heap-space\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"PropertyData\").getOrCreate()\n",
    "spark\n",
    "\n",
    "zip_url = \"https://www.tad.org/content/data-download/PropertyData(Delimited).ZIP\"\n",
    "source_file = \"/tmp/spark/PropertyData.txt\"\n",
    "download_dir = \"/tmp/spark/\"\n",
    "\n",
    "if os.path.isfile(source_file):\n",
    "    print(\"Using existing property file\")\n",
    "else:\n",
    "    r = requests.get(zip_url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(download_dir)\n",
    "\n",
    "df = spark.read.csv(source_file, sep=\"|\", header=True, inferSchema=True)\n",
    "df = df.withColumn('id', monotonically_increasing_id())\n",
    "df = df[['id'] + df.columns[:-1]]   # move id column to front\n",
    "for name, dtype in df.dtypes:       # trim all string columns\n",
    "    if dtype == \"string\":\n",
    "        df = df.withColumn(name, trim(col(name)))\n",
    "\n",
    "\n",
    "# Replace missing string values for columns that are used in logistic regression later.\n",
    "# (Even if you convert them to IDX and drop them, you still get \"Cannot have an empty string for name\" error!!)\n",
    "# see: https://stackoverflow.com/questions/33089781/spark-dataframe-handing-empty-string-in-onehotencoder\n",
    "df = df.withColumn('Swimming_Pool_Ind', when(col('Swimming_Pool_Ind') == 'X', 'Y').otherwise('N'))\n",
    "\n",
    "# convert certain string columns to idx for use with logistic regression later on\n",
    "string_to_idx_columns = ['Swimming_Pool_Ind', 'Central_Heat_Ind', 'Central_Air_Ind']\n",
    "for col_name in string_to_idx_columns:\n",
    "    si = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_idx\")\n",
    "    df = si.fit(df).transform(df)    \n",
    "\n",
    "\n",
    "# convert the Appraised_Value to integer\n",
    "df = df.withColumn('Appraised_Value_Int', df['Appraised_Value'].cast('int'))\n",
    "\n",
    "# drop specific columns we don't need going forward, including 'Appraised_Value' ...\n",
    "drop_cols = ['Sequence_No', 'Record_Type', 'PIDN', 'Owner_Name', 'Owner_Address', 'Owner_CityState', 'Owner_Zip4',\n",
    "             'Owner_CRRT', 'Situs_Address', 'TAD_Map', 'MAPSCO', 'Exemption_Code', 'State_Use_Code', 'LegalDescription',\n",
    "             'Notice_Date', 'Deed_Date', 'Deed_Book', 'Appraisal_Date', 'Deed_Page', 'ARB_Indicator', 'From_Accts',\n",
    "             'GIS_Link', 'Instrument_No', 'Overlap_Flag', 'Num_Bedrooms', 'Num_Bathrooms', 'Structure_Count', 'Ag_Code',\n",
    "             'Appraisal_Year', 'Owner_Zip', 'Appraised_Value']\n",
    "df = df.drop(*drop_cols)\n",
    "\n",
    "# Apply One-Hot encoding to _idx to get _bin for logistic columns\n",
    "for col_name in ['Swimming_Pool_Ind', 'Central_Air_Ind', 'Central_Heat_Ind']:\n",
    "    enc = OneHotEncoder(inputCols=[f\"{col_name}_idx\"], outputCols=[f\"{col_name}_bin\"])\n",
    "    df = enc.fit(df).transform(df)\n",
    "\n",
    "# Save as a parquet file\n",
    "df.write.parquet(download_dir + \"tarrant-tx.parquet\", mode=\"overwrite\")\n",
    "print('Parquet files created. Use tarrant-tx_multi-linear.ipynb and tarrant-tx_logistic.ipynb for analysis.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! Optional step to see how many rows should be removed for incomplete data\n",
    "# Inspect dataframe for missing values\n",
    "\n",
    "# from pyspark.sql.functions import sum, col\n",
    "# missing_value_counts = df.select([sum(col(column).isNull().cast(\"int\")).alias(column) for column in df.columns])\n",
    "# missing_value_counts.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
