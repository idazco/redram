{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File: data-samples/tarrant-tx.csv  \n",
    "Source: https://www.tad.org/data-reports\n",
    "\n",
    "### Get the data from the website into a data-frame\n",
    "1. The only clean-up we're doing here is making sure the string data is trimmed\n",
    "2. The part is idempotent - after running it at least once, delete /tmp/spark/PropertyData.txt to have it use fresh data from the download source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing property file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>RP</th><th>Appraisal_Year</th><th>Account_Num</th><th>Record_Type</th><th>Owner_Zip</th><th>Property_Class</th><th>State_Use_Code</th><th>County</th><th>City</th><th>School</th><th>Num_Special_Dist</th><th>Spec1</th><th>Spec2</th><th>Spec3</th><th>Spec4</th><th>Spec5</th><th>Land_Value</th><th>Improvement_Value</th><th>Total_Value</th><th>Garage_Capacity</th><th>Num_Bedrooms</th><th>Num_Bathrooms</th><th>Year_Built</th><th>Living_Area</th><th>Swimming_Pool_Ind</th><th>Ag_Code</th><th>Land_Acres</th><th>Land_SqFt</th><th>Ag_Acres</th><th>Ag_Value</th><th>Central_Heat_Ind</th><th>Central_Air_Ind</th><th>Structure_Count</th><th>Appraised_Value</th></tr>\n",
       "<tr><td>0</td><td>C</td><td>2023</td><td>51</td><td>AAAA</td><td>76102</td><td>C1C</td><td>C1</td><td>220</td><td>26</td><td>905</td><td>4</td><td>223</td><td>224</td><td>225</td><td>0</td><td>601</td><td>450000.0</td><td>0.0</td><td>450000.0</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td></td><td></td><td>0.1147</td><td>5000.0</td><td>0.0</td><td>0.0</td><td>N</td><td>N</td><td>0.0</td><td>450000.0</td></tr>\n",
       "<tr><td>1</td><td>C</td><td>2023</td><td>78</td><td>AAAA</td><td>76102</td><td>C1C</td><td>C1</td><td>220</td><td>26</td><td>905</td><td>4</td><td>223</td><td>224</td><td>225</td><td>0</td><td>601</td><td>450000.0</td><td>0.0</td><td>450000.0</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td></td><td></td><td>0.1147</td><td>5000.0</td><td>0.0</td><td>0.0</td><td>N</td><td>N</td><td>0.0</td><td>450000.0</td></tr>\n",
       "<tr><td>2</td><td>C</td><td>2023</td><td>86</td><td>AAAA</td><td>76102</td><td>C1C</td><td>C1</td><td>220</td><td>26</td><td>905</td><td>4</td><td>223</td><td>224</td><td>225</td><td>0</td><td>601</td><td>675000.0</td><td>0.0</td><td>675000.0</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td></td><td></td><td>0.1721</td><td>7500.0</td><td>0.0</td><td>0.0</td><td>N</td><td>N</td><td>0.0</td><td>675000.0</td></tr>\n",
       "<tr><td>3</td><td>C</td><td>2023</td><td>94</td><td>AAAA</td><td>76102</td><td>C1C</td><td>C1</td><td>220</td><td>26</td><td>905</td><td>4</td><td>223</td><td>224</td><td>225</td><td>0</td><td>601</td><td>225000.0</td><td>0.0</td><td>225000.0</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td></td><td></td><td>0.0573</td><td>2500.0</td><td>0.0</td><td>0.0</td><td>N</td><td>N</td><td>0.0</td><td>225000.0</td></tr>\n",
       "<tr><td>4</td><td>C</td><td>2023</td><td>108</td><td>AAAA</td><td>76102</td><td>C1C</td><td>C1</td><td>220</td><td>26</td><td>905</td><td>4</td><td>223</td><td>224</td><td>225</td><td>0</td><td>601</td><td>1080000.0</td><td>0.0</td><td>1080000.0</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td></td><td></td><td>0.2754</td><td>12000.0</td><td>0.0</td><td>0.0</td><td>N</td><td>N</td><td>0.0</td><td>1080000.0</td></tr>\n",
       "<tr><td>5</td><td>C</td><td>2023</td><td>116</td><td>AAAA</td><td>76102</td><td>F1</td><td>F1</td><td>220</td><td>26</td><td>905</td><td>4</td><td>223</td><td>224</td><td>225</td><td>0</td><td>601</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td></td><td></td><td>0.2295</td><td>10000.0</td><td>0.0</td><td>0.0</td><td>N</td><td>N</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>6</td><td>C</td><td>2023</td><td>124</td><td>AAAA</td><td>76102</td><td>F1</td><td>F1</td><td>220</td><td>26</td><td>905</td><td>4</td><td>223</td><td>224</td><td>225</td><td>0</td><td>601</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td></td><td></td><td>0.0688</td><td>3000.0</td><td>0.0</td><td>0.0</td><td>N</td><td>N</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>7</td><td>C</td><td>2023</td><td>132</td><td>AAAA</td><td></td><td>F1</td><td>F1</td><td>220</td><td>26</td><td>905</td><td>4</td><td>223</td><td>224</td><td>225</td><td>0</td><td>601</td><td>1350000.0</td><td>2526017.0</td><td>3876017.0</td><td>0.0</td><td>0</td><td>0</td><td>1917.0</td><td>72779.0</td><td></td><td></td><td>0.3443</td><td>15000.0</td><td>0.0</td><td>0.0</td><td>N</td><td>N</td><td>0.0</td><td>3876017.0</td></tr>\n",
       "<tr><td>8</td><td>C</td><td>2023</td><td>140</td><td>AAAA</td><td>76102</td><td>C2C</td><td>C2</td><td>220</td><td>26</td><td>905</td><td>4</td><td>223</td><td>224</td><td>225</td><td>0</td><td>601</td><td>630090.0</td><td>1000.0</td><td>631090.0</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td></td><td></td><td>0.1607</td><td>7001.0</td><td>0.0</td><td>0.0</td><td>N</td><td>N</td><td>0.0</td><td>631090.0</td></tr>\n",
       "<tr><td>9</td><td>C</td><td>2023</td><td>159</td><td>AAAA</td><td>76102</td><td>F1</td><td>F1</td><td>220</td><td>26</td><td>905</td><td>4</td><td>223</td><td>224</td><td>225</td><td>0</td><td>601</td><td>522000.0</td><td>199000.0</td><td>721000.0</td><td>0.0</td><td>0</td><td>0</td><td>1920.0</td><td>12400.0</td><td></td><td></td><td>0.1331</td><td>5800.0</td><td>0.0</td><td>0.0</td><td>N</td><td>N</td><td>0.0</td><td>721000.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---+--------------+-----------+-----------+---------+--------------+--------------+------+----+------+----------------+-----+-----+-----+-----+-----+----------+-----------------+-----------+---------------+------------+-------------+----------+-----------+-----------------+-------+----------+---------+--------+--------+----------------+---------------+---------------+---------------+\n",
       "| id| RP|Appraisal_Year|Account_Num|Record_Type|Owner_Zip|Property_Class|State_Use_Code|County|City|School|Num_Special_Dist|Spec1|Spec2|Spec3|Spec4|Spec5|Land_Value|Improvement_Value|Total_Value|Garage_Capacity|Num_Bedrooms|Num_Bathrooms|Year_Built|Living_Area|Swimming_Pool_Ind|Ag_Code|Land_Acres|Land_SqFt|Ag_Acres|Ag_Value|Central_Heat_Ind|Central_Air_Ind|Structure_Count|Appraised_Value|\n",
       "+---+---+--------------+-----------+-----------+---------+--------------+--------------+------+----+------+----------------+-----+-----+-----+-----+-----+----------+-----------------+-----------+---------------+------------+-------------+----------+-----------+-----------------+-------+----------+---------+--------+--------+----------------+---------------+---------------+---------------+\n",
       "|  0|  C|          2023|         51|       AAAA|    76102|           C1C|            C1|   220|  26|   905|               4|  223|  224|  225|    0|  601|  450000.0|              0.0|   450000.0|            0.0|           0|            0|       0.0|        0.0|                 |       |    0.1147|   5000.0|     0.0|     0.0|               N|              N|            0.0|       450000.0|\n",
       "|  1|  C|          2023|         78|       AAAA|    76102|           C1C|            C1|   220|  26|   905|               4|  223|  224|  225|    0|  601|  450000.0|              0.0|   450000.0|            0.0|           0|            0|       0.0|        0.0|                 |       |    0.1147|   5000.0|     0.0|     0.0|               N|              N|            0.0|       450000.0|\n",
       "|  2|  C|          2023|         86|       AAAA|    76102|           C1C|            C1|   220|  26|   905|               4|  223|  224|  225|    0|  601|  675000.0|              0.0|   675000.0|            0.0|           0|            0|       0.0|        0.0|                 |       |    0.1721|   7500.0|     0.0|     0.0|               N|              N|            0.0|       675000.0|\n",
       "|  3|  C|          2023|         94|       AAAA|    76102|           C1C|            C1|   220|  26|   905|               4|  223|  224|  225|    0|  601|  225000.0|              0.0|   225000.0|            0.0|           0|            0|       0.0|        0.0|                 |       |    0.0573|   2500.0|     0.0|     0.0|               N|              N|            0.0|       225000.0|\n",
       "|  4|  C|          2023|        108|       AAAA|    76102|           C1C|            C1|   220|  26|   905|               4|  223|  224|  225|    0|  601| 1080000.0|              0.0|  1080000.0|            0.0|           0|            0|       0.0|        0.0|                 |       |    0.2754|  12000.0|     0.0|     0.0|               N|              N|            0.0|      1080000.0|\n",
       "|  5|  C|          2023|        116|       AAAA|    76102|            F1|            F1|   220|  26|   905|               4|  223|  224|  225|    0|  601|       0.0|              0.0|        0.0|            0.0|           0|            0|       0.0|        0.0|                 |       |    0.2295|  10000.0|     0.0|     0.0|               N|              N|            0.0|            0.0|\n",
       "|  6|  C|          2023|        124|       AAAA|    76102|            F1|            F1|   220|  26|   905|               4|  223|  224|  225|    0|  601|       0.0|              0.0|        0.0|            0.0|           0|            0|       0.0|        0.0|                 |       |    0.0688|   3000.0|     0.0|     0.0|               N|              N|            0.0|            0.0|\n",
       "|  7|  C|          2023|        132|       AAAA|         |            F1|            F1|   220|  26|   905|               4|  223|  224|  225|    0|  601| 1350000.0|        2526017.0|  3876017.0|            0.0|           0|            0|    1917.0|    72779.0|                 |       |    0.3443|  15000.0|     0.0|     0.0|               N|              N|            0.0|      3876017.0|\n",
       "|  8|  C|          2023|        140|       AAAA|    76102|           C2C|            C2|   220|  26|   905|               4|  223|  224|  225|    0|  601|  630090.0|           1000.0|   631090.0|            0.0|           0|            0|       0.0|        0.0|                 |       |    0.1607|   7001.0|     0.0|     0.0|               N|              N|            0.0|       631090.0|\n",
       "|  9|  C|          2023|        159|       AAAA|    76102|            F1|            F1|   220|  26|   905|               4|  223|  224|  225|    0|  601|  522000.0|         199000.0|   721000.0|            0.0|           0|            0|    1920.0|    12400.0|                 |       |    0.1331|   5800.0|     0.0|     0.0|               N|              N|            0.0|       721000.0|\n",
       "+---+---+--------------+-----------+-----------+---------+--------------+--------------+------+----+------+----------------+-----+-----+-----+-----+-----+----------+-----------------+-----------+---------------+------------+-------------+----------+-----------+-----------------+-------+----------+---------+--------+--------+----------------+---------------+---------------+---------------+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, zipfile, io, os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id, trim, col\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"PropertyData\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
    "spark\n",
    "zip_url = \"https://www.tad.org/content/data-download/PropertyData(Delimited).ZIP\"\n",
    "source_file = \"/tmp/spark/PropertyData.txt\"\n",
    "\n",
    "if os.path.isfile(source_file):\n",
    "    print(\"Using existing property file\")\n",
    "else:\n",
    "    download_url = \"/tmp/spark/\"\n",
    "    r = requests.get(zip_url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(download_url)\n",
    "\n",
    "df = spark.read.csv(source_file, sep=\"|\", header=True, inferSchema=True)\n",
    "df = df.withColumn('id', monotonically_increasing_id())\n",
    "df = df[['id'] + df.columns[:-1]]   # move id column to front\n",
    "for name, dtype in df.dtypes:       # trim all string columns\n",
    "    if dtype == \"string\":\n",
    "        df = df.withColumn(name, trim(col(name)))\n",
    "\n",
    "# drop columns we don't need for this\n",
    "drop_cols = ['Sequence_No', 'Record_Type', 'PIDN', 'Owner_Name', 'Owner_Address', 'Owner_CityState', 'Owner_Zip4',\n",
    "             'Owner_CRRT', 'Situs_Address', 'TAD_Map', 'MAPSCO', 'Exemption_Code', 'State_Use_Code', 'LegalDescription',\n",
    "             'Notice_Date', 'Deed_Date', 'Deed_Book', 'Appraisal_Date', 'Deed_Page', 'ARB_Indicator', 'From_Accts',\n",
    "             'GIS_Link', 'Instrument_No', 'Overlap_Flag']\n",
    "df = df.drop(*drop_cols)\n",
    "\n",
    "# optionally put the data into a view for SQL queries\n",
    "df.createOrReplaceTempView(\"data\")\n",
    "spark.sql('select * from data limit 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Property_Class</th><th>State_Use_Code</th></tr>\n",
       "<tr><td>J3</td><td>J3</td></tr>\n",
       "<tr><td>F1</td><td>F1</td></tr>\n",
       "<tr><td>EC</td><td>EC</td></tr>\n",
       "<tr><td>AC</td><td>AC</td></tr>\n",
       "<tr><td>C1C</td><td>C1</td></tr>\n",
       "<tr><td>BC</td><td>BC</td></tr>\n",
       "<tr><td>G1</td><td>G1</td></tr>\n",
       "<tr><td>J2</td><td>J2</td></tr>\n",
       "<tr><td>J4</td><td>J4</td></tr>\n",
       "<tr><td>J6</td><td>J6</td></tr>\n",
       "<tr><td>ROC</td><td>RO</td></tr>\n",
       "<tr><td>J1</td><td>J1</td></tr>\n",
       "<tr><td>X</td><td>X</td></tr>\n",
       "<tr><td>C2C</td><td>C2</td></tr>\n",
       "<tr><td>F2</td><td>F2</td></tr>\n",
       "<tr><td>J8</td><td>J8</td></tr>\n",
       "<tr><td>J7</td><td>J7</td></tr>\n",
       "<tr><td>J5</td><td>J5</td></tr>\n",
       "<tr><td>J3C</td><td>J3</td></tr>\n",
       "<tr><td>J6C</td><td>J6</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------+--------------+\n",
       "|Property_Class|State_Use_Code|\n",
       "+--------------+--------------+\n",
       "|            J3|            J3|\n",
       "|            F1|            F1|\n",
       "|            EC|            EC|\n",
       "|            AC|            AC|\n",
       "|           C1C|            C1|\n",
       "|            BC|            BC|\n",
       "|            G1|            G1|\n",
       "|            J2|            J2|\n",
       "|            J4|            J4|\n",
       "|            J6|            J6|\n",
       "|           ROC|            RO|\n",
       "|            J1|            J1|\n",
       "|             X|             X|\n",
       "|           C2C|            C2|\n",
       "|            F2|            F2|\n",
       "|            J8|            J8|\n",
       "|            J7|            J7|\n",
       "|            J5|            J5|\n",
       "|           J3C|            J3|\n",
       "|           J6C|            J6|\n",
       "+--------------+--------------+"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select Property_Class, State_Use_Code  from data group by 1, 2 limit 20')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training.\n",
    "First, use the numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])\n",
    "numerical_features = ['Appraisal_Year', 'County', 'City', 'School', 'Num_Special_Dist', 'Spec1', 'Spec2',\n",
    "                      'Spec3', 'Spec4', 'Spec5', 'Land_Value', 'Improvement_Value', 'Total_Value',\n",
    "                      'Garage_Capacity', 'Num_Bedrooms', 'Num_Bathrooms', 'Year_Built', 'Living_Area',\n",
    "                      'Land_Acres', 'Land_SqFt', 'Ag_Acres', 'Ag_Value', 'Structure_Count']\n",
    "\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler, StandardScaler\n",
    "\n",
    "imputer = Imputer(inputCols=numerical_features, outputCols=numerical_features)\n",
    "imputer = imputer.fit(train)\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(test)\n",
    "\n",
    "numerical_VA = VectorAssembler(inputCols=numerical_features, outputCol='numerical_vector')\n",
    "train = numerical_VA.transform(train)\n",
    "test = numerical_VA.transform(test)\n",
    "\n",
    "scaler = StandardScaler(inputCol='numerical_vector', outputCol='scaled_vector', withStd=True, withMean=True)\n",
    "scaler = scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, use indexed categorical (string) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "pool_catidx = StringIndexer(inputCol='Swimming_Pool_Ind', outputCol='Swimming_Pool_Ind_catidx')\n",
    "pool_catidx = pool_catidx.fit(train)\n",
    "train = pool_catidx.transform(train)\n",
    "test = pool_catidx.transform(test)\n",
    "# to-do: add other categories as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Cannot have an empty string for name.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ohe \u001b[39m=\u001b[39m OneHotEncoder(inputCol\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSwimming_Pool_Ind_catidx\u001b[39m\u001b[39m'\u001b[39m, outputCol\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSwimming_Pool_Ind_one_hot\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m ohe \u001b[39m=\u001b[39m ohe\u001b[39m.\u001b[39;49mfit(train)\n\u001b[1;32m      3\u001b[0m \u001b[39m# train = ohe.transform(train)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# test = ohe.transform(test)\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/git/idazco/redram/spark/.venv/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/git/idazco/redram/spark/.venv/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/Projects/git/idazco/redram/spark/.venv/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m~/Projects/git/idazco/redram/spark/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Projects/git/idazco/redram/spark/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Cannot have an empty string for name."
     ]
    }
   ],
   "source": [
    "ohe = OneHotEncoder(inputCol='Swimming_Pool_Ind_catidx', outputCol='Swimming_Pool_Ind_one_hot')\n",
    "ohe = ohe.fit(train)\n",
    "# train = ohe.transform(train)\n",
    "# test = ohe.transform(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
