{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data from the website and prepare it\n",
    "\n",
    "File: PropertyData.txt  \n",
    "Source: https://www.tad.org (Tarrant Appraisal District, TX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io, os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id, trim, col, udf, when\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# memory issues .. see https://stackoverflow.com/questions/21138751/spark-java-lang-outofmemoryerror-java-heap-space\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "        .config(\"spark.executor.memory\", \"26g\")\\\n",
    "        .config(\"spark.driver.memory\", \"26g\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\", True).config(\"spark.memory.offHeap.size\",\"16g\")\\\n",
    "        .appName(\"PropertyData\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
    "spark\n",
    "zip_url = \"https://www.tad.org/content/data-download/PropertyData(Delimited).ZIP\"\n",
    "source_file = \"/tmp/spark/PropertyData.txt\"\n",
    "\n",
    "if os.path.isfile(source_file):\n",
    "    print(\"Using existing property file\")\n",
    "else:\n",
    "    download_url = \"/tmp/spark/\"\n",
    "    r = requests.get(zip_url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(download_url)\n",
    "\n",
    "df = spark.read.csv(source_file, sep=\"|\", header=True, inferSchema=True)\n",
    "df = df.withColumn('id', monotonically_increasing_id())\n",
    "df = df[['id'] + df.columns[:-1]]   # move id column to front\n",
    "for name, dtype in df.dtypes:       # trim all string columns\n",
    "    if dtype == \"string\":\n",
    "        df = df.withColumn(name, trim(col(name)))\n",
    "\n",
    "\n",
    "# Replace missing string values for columns that are used in logistic regression later.\n",
    "# (Even if you convert them to IDX and drop them, you still get \"Cannot have an empty string for name\" error!!)\n",
    "# see: https://stackoverflow.com/questions/33089781/spark-dataframe-handing-empty-string-in-onehotencoder\n",
    "df = df.withColumn('Swimming_Pool_Ind', when(col('Swimming_Pool_Ind') == 'X', 'Y').otherwise('N'))\n",
    "\n",
    "# convert certain string columns to idx for use with logistic regression later on\n",
    "string_to_idx_columns = ['Swimming_Pool_Ind', 'Central_Heat_Ind', 'Central_Air_Ind']\n",
    "for col_name in string_to_idx_columns:\n",
    "    si = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_idx\")\n",
    "    df = si.fit(df).transform(df)    \n",
    "\n",
    "\n",
    "# convert the Appraised_Value to integer\n",
    "df = df.withColumn('Appraised_Value_Int', df['Appraised_Value'].cast('int'))\n",
    "\n",
    "# drop specific columns we don't need going forward, including 'Appraised_Value' ...\n",
    "drop_cols = ['Sequence_No', 'Record_Type', 'PIDN', 'Owner_Name', 'Owner_Address', 'Owner_CityState', 'Owner_Zip4',\n",
    "             'Owner_CRRT', 'Situs_Address', 'TAD_Map', 'MAPSCO', 'Exemption_Code', 'State_Use_Code', 'LegalDescription',\n",
    "             'Notice_Date', 'Deed_Date', 'Deed_Book', 'Appraisal_Date', 'Deed_Page', 'ARB_Indicator', 'From_Accts',\n",
    "             'GIS_Link', 'Instrument_No', 'Overlap_Flag', 'Num_Bedrooms', 'Num_Bathrooms', 'Structure_Count', 'Ag_Code',\n",
    "             'Appraisal_Year', 'Owner_Zip', 'Appraised_Value']\n",
    "df = df.drop(*drop_cols)\n",
    "\n",
    "# udpdate the view for SQL queries\n",
    "# df.createOrReplaceTempView(\"data\")\n",
    "# spark.sql('select * from data limit 10')\n",
    "\n",
    "# Define a UDF to calculate the percentage difference\n",
    "from pyspark.sql.types import DoubleType\n",
    "percent_diff_udf = udf(lambda new_value, old_value: round(((new_value - old_value) / old_value) * 100, 0) if old_value != 0 else None,\n",
    "                       DoubleType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! Optional step to see how many rows should be removed for incomplete data\n",
    "\n",
    "# Inspect dataframe for missing values\n",
    "from pyspark.sql.functions import sum, col\n",
    "missing_value_counts = df.select([sum(col(column).isNull().cast(\"int\")).alias(column) for column in df.columns])\n",
    "missing_value_counts.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "subset = [ 'Total_Value', 'Appraised_Value_Int',   \n",
    "    'Central_Heat_Ind_idx', 'Central_Air_Ind_idx', 'Swimming_Pool_Ind_idx'\n",
    "]\n",
    "\n",
    "logistic_df = df.select(*subset).limit(10)  # <----- 10 works, 100 causes \"java.lang.NegativeArraySizeException\" ?\n",
    "\n",
    "# Apply One-Hot encoding to _idx to get _bin for logistic columns\n",
    "for col_name in ['Swimming_Pool_Ind', 'Central_Air_Ind', 'Central_Heat_Ind']:\n",
    "    enc = OneHotEncoder(inputCols=[f\"{col_name}_idx\"], outputCols=[f\"{col_name}_bin\"])\n",
    "    logistic_df = enc.fit(logistic_df).transform(logistic_df)\n",
    "\n",
    "logistic_features = [\n",
    "    'Total_Value', 'Central_Heat_Ind_idx', 'Central_Air_Ind_idx', 'Swimming_Pool_Ind_idx',\n",
    "]\n",
    "\n",
    "model_logistic = LogisticRegression(featuresCol=\"features\", labelCol=\"Appraised_Value_Int\")\n",
    "\n",
    "# Train\n",
    "train_logistic, test_logistic = logistic_df.randomSplit([0.7, 0.3])\n",
    "assembler = VectorAssembler(inputCols=logistic_features, outputCol='features')\n",
    "train_logistic = assembler.transform(train_logistic)\n",
    "test_logistic = assembler.transform(test_logistic)\n",
    "logistic_trained_model = model_logistic.fit(train_logistic)\n",
    "\n",
    "# Evaluate\n",
    "logistic_predictions = logistic_trained_model.transform(test_logistic)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='Appraised_Value_Int', rawPredictionCol='prediction')\n",
    "accuracy = evaluator.evaluate(logistic_predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Linear Regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define the feature columns\n",
    "numerical_features = ['Spec1', 'Spec2', 'Spec3', 'Spec4', 'Spec5', 'Total_Value']\n",
    "\n",
    "# Drop rows containing missing values in the columns for features data\n",
    "cleaned_numeric_df = df.dropna(subset=numerical_features)\n",
    "\n",
    "train_linear, test_linear = cleaned_numeric_df.randomSplit([0.7, 0.3])\n",
    "model_linear = LinearRegression(featuresCol='features', labelCol='Appraised_Value_Int')\n",
    "assembler = VectorAssembler(inputCols=numerical_features, outputCol='features')\n",
    "\n",
    "# Train\n",
    "train_linear = assembler.transform(train_linear)\n",
    "test_linear = assembler.transform(test_linear)\n",
    "linear_trained_model = model_linear.fit(train_linear)\n",
    "\n",
    "# Evaluate\n",
    "linear_predictions = linear_trained_model.transform(test_linear)\n",
    "evaluator = RegressionEvaluator(labelCol='Appraised_Value_Int', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(linear_predictions)\n",
    "\n",
    "# cast the predictions to integer and show the difference as integer and perentage\n",
    "linear_predictions = linear_predictions.withColumn('prediction_int', linear_predictions['prediction'].cast('int'))\n",
    "linear_predictions = linear_predictions.drop('prediction')\n",
    "linear_predictions = linear_predictions.withColumn('diff', col('Appraised_Value_Int') - col('prediction_int'))\n",
    "linear_predictions = linear_predictions.withColumn('diff_percent', percent_diff_udf(col('prediction_int'), col('Appraised_Value_Int')))\n",
    "\n",
    "# create the view with summary data\n",
    "linear_predictions.select('Account_Num', 'prediction_int', 'diff', 'diff_percent').createOrReplaceTempView(\"linear\")\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE):\", round(rmse, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at some of the data\n",
    "# we group the diff_percent values so there aren't too many records to graph\n",
    "# also, here we are just looking at residential properties\n",
    "query = \"\"\"\n",
    "with summary as (\n",
    "  with src as (\n",
    "    select FLOOR(CAST(diff_percent as INT) / 20) * 20 AS diff_group\n",
    "    from linear \n",
    "    join data on data.Account_Num = linear.Account_Num\n",
    "    where data.RP = 'R' \n",
    "      and Appraised_Value_Int > 20000\n",
    "  )\n",
    "  select count(diff_group) as count, diff_group from src group by 2 order by 2 desc\n",
    ")\n",
    "select count, diff_group from summary where count > 100\n",
    "\"\"\"\n",
    "res = spark.sql(query)\n",
    "# res.show()\n",
    "\n",
    "# graph it show how the predictions differ from the current appraisal values\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pandas_df = res.toPandas()\n",
    "plt.bar(pandas_df['diff_group'], pandas_df['count'], color='blue', width=8)\n",
    "plt.xlabel('diff_group')\n",
    "plt.ylabel('count')\n",
    "plt.title('Prediction Diff (longest line should be over \"0\")') # .. which would indicate more accurate prediction\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
