{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File: data-samples/tarrant-tx.csv  \n",
    "Source: https://www.tad.org/data-reports\n",
    "\n",
    "### Get the data from the website into a data-frame\n",
    "1. The only clean-up we're doing here is making sure the string data is trimmed\n",
    "2. The part is idempotent - after running it at least once, delete /tmp/spark/PropertyData.txt to have it use fresh data from the download source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io, os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id, trim, col\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"PropertyData\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
    "spark\n",
    "zip_url = \"https://www.tad.org/content/data-download/PropertyData(Delimited).ZIP\"\n",
    "source_file = \"/tmp/spark/PropertyData.txt\"\n",
    "\n",
    "if os.path.isfile(source_file):\n",
    "    print(\"Using existing property file\")\n",
    "else:\n",
    "    download_url = \"/tmp/spark/\"\n",
    "    r = requests.get(zip_url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(download_url)\n",
    "\n",
    "df = spark.read.csv(source_file, sep=\"|\", header=True, inferSchema=True)\n",
    "df = df.withColumn('id', monotonically_increasing_id())\n",
    "df = df[['id'] + df.columns[:-1]]   # move id column to front\n",
    "for name, dtype in df.dtypes:       # trim all string columns\n",
    "    if dtype == \"string\":\n",
    "        df = df.withColumn(name, trim(col(name)))\n",
    "\n",
    "# drop columns we don't need for this\n",
    "drop_cols = ['Sequence_No', 'Record_Type', 'PIDN', 'Owner_Name', 'Owner_Address', 'Owner_CityState', 'Owner_Zip4',\n",
    "             'Owner_CRRT', 'Situs_Address', 'TAD_Map', 'MAPSCO', 'Exemption_Code', 'State_Use_Code', 'LegalDescription',\n",
    "             'Notice_Date', 'Deed_Date', 'Deed_Book', 'Appraisal_Date', 'Deed_Page', 'ARB_Indicator', 'From_Accts',\n",
    "             'GIS_Link', 'Instrument_No', 'Overlap_Flag', 'Num_Bedrooms', 'Num_Bathrooms', 'Structure_Count', 'Ag_Code',\n",
    "             'Appraisal_Year']\n",
    "df = df.drop(*drop_cols)\n",
    "\n",
    "# optionally put the data into a view for SQL queries\n",
    "df.createOrReplaceTempView(\"data\")\n",
    "spark.sql('select * from data limit 10')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training.\n",
    "First, use the numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])\n",
    "numerical_features = ['County', 'City', 'School', 'Num_Special_Dist', 'Spec1', 'Spec2',\n",
    "                      'Spec3', 'Spec4', 'Spec5', 'Land_Value', 'Improvement_Value', 'Total_Value',\n",
    "                      'Garage_Capacity', 'Year_Built', 'Living_Area', 'Land_Acres', 'Land_SqFt', 'Ag_Acres',\n",
    "                      'Ag_Value']\n",
    "\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler, StandardScaler\n",
    "\n",
    "imputer = Imputer(inputCols=numerical_features, outputCols=numerical_features)\n",
    "imputer = imputer.fit(train)\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(test)\n",
    "\n",
    "numerical_VA = VectorAssembler(inputCols=numerical_features, outputCol='numerical_vector')\n",
    "train = numerical_VA.transform(train)\n",
    "test = numerical_VA.transform(test)\n",
    "\n",
    "scaler = StandardScaler(inputCol='numerical_vector', outputCol='scaled_vector', withStd=True, withMean=True)\n",
    "scaler = scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, use indexed categorical (string) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "pool_catidx = StringIndexer(inputCol='Swimming_Pool_Ind', outputCol='Swimming_Pool_Ind_catidx')\n",
    "pool_catidx = pool_catidx.fit(train)\n",
    "train = pool_catidx.transform(train)\n",
    "test = pool_catidx.transform(test)\n",
    "# to-do: add other categories as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(inputCol='Swimming_Pool_Ind_catidx', outputCol='Swimming_Pool_Ind_one_hot')\n",
    "ohe = ohe.fit(train)\n",
    "# train = ohe.transform(train)\n",
    "# test = ohe.transform(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
