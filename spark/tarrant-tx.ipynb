{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data from the website and prepare it\n",
    "\n",
    "File: PropertyData.txt\n",
    "Source: https://www.tad.org (Tarrant Appraisal District, TX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io, os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id, trim, col, udf\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"PropertyData\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
    "spark\n",
    "zip_url = \"https://www.tad.org/content/data-download/PropertyData(Delimited).ZIP\"\n",
    "source_file = \"/tmp/spark/PropertyData.txt\"\n",
    "\n",
    "if os.path.isfile(source_file):\n",
    "    print(\"Using existing property file\")\n",
    "else:\n",
    "    download_url = \"/tmp/spark/\"\n",
    "    r = requests.get(zip_url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(download_url)\n",
    "\n",
    "df = spark.read.csv(source_file, sep=\"|\", header=True, inferSchema=True)\n",
    "df = df.withColumn('id', monotonically_increasing_id())\n",
    "df = df[['id'] + df.columns[:-1]]   # move id column to front\n",
    "for name, dtype in df.dtypes:       # trim all string columns\n",
    "    if dtype == \"string\":\n",
    "        df = df.withColumn(name, trim(col(name)))\n",
    "\n",
    "# convert certain string columns to number for use with regression\n",
    "string_to_idx_columns = ['Swimming_Pool_Ind', 'Central_Heat_Ind', 'Central_Air_Ind']\n",
    "for col_name in string_to_idx_columns:\n",
    "    si = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_idx\")\n",
    "    si = si.fit(df)\n",
    "    df = si.transform(df)\n",
    "\n",
    "# convert the Appraised_Value to integer\n",
    "df = df.withColumn('Appraised_Value_Int', df['Appraised_Value'].cast('int'))\n",
    "\n",
    "# drop specific columns we don't need going forward ...\n",
    "drop_cols = ['Sequence_No', 'Record_Type', 'PIDN', 'Owner_Name', 'Owner_Address', 'Owner_CityState', 'Owner_Zip4',\n",
    "             'Owner_CRRT', 'Situs_Address', 'TAD_Map', 'MAPSCO', 'Exemption_Code', 'State_Use_Code', 'LegalDescription',\n",
    "             'Notice_Date', 'Deed_Date', 'Deed_Book', 'Appraisal_Date', 'Deed_Page', 'ARB_Indicator', 'From_Accts',\n",
    "             'GIS_Link', 'Instrument_No', 'Overlap_Flag', 'Num_Bedrooms', 'Num_Bathrooms', 'Structure_Count', 'Ag_Code',\n",
    "             'Appraisal_Year', 'Owner_Zip', 'Appraised_Value']\n",
    "df = df.drop(*drop_cols)\n",
    "\n",
    "# udpdate the view for SQL queries\n",
    "df.createOrReplaceTempView(\"data\")\n",
    "# spark.sql('select * from data limit 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to calculate the percentage difference\n",
    "from pyspark.sql.types import DoubleType\n",
    "percent_diff_udf = udf(lambda new_value, old_value: round(((new_value - old_value) / old_value) * 100, 1) if old_value != 0 else None,\n",
    "                       DoubleType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature columns\n",
    "numerical_features = ['County', 'City', 'School', 'Num_Special_Dist', 'Spec1', 'Spec2',\n",
    "                      'Spec3', 'Spec4', 'Spec5', 'Land_Value', 'Improvement_Value', 'Total_Value',\n",
    "                      'Garage_Capacity', 'Year_Built', 'Living_Area', 'Land_Acres', 'Land_SqFt', 'Ag_Acres',\n",
    "                      'Ag_Value', 'Swimming_Pool_Ind_idx', 'Central_Heat_Ind_idx', 'Central_Air_Ind_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dataframe for missing values\n",
    "from pyspark.sql.functions import sum, col\n",
    "missing_value_counts = df.select([sum(col(column).isNull().cast(\"int\")).alias(column) for column in df.columns])\n",
    "missing_value_counts.show()\n",
    "\n",
    "# Drop rows containing missing values in the columns for features data\n",
    "cleaned_df = df.dropna(subset=numerical_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start creating the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "train_linear, test_linear = cleaned_df.randomSplit([0.7, 0.3], seed=42)\n",
    "model_linear = LinearRegression(featuresCol='features', labelCol='Appraised_Value_Int')\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numerical_features, outputCol='features')\n",
    "train_linear = assembler.transform(train_linear)\n",
    "test_linear = assembler.transform(test_linear)\n",
    "\n",
    "# Train the model\n",
    "trained_model = model_linear.fit(train_linear)\n",
    "\n",
    "# Evaluate the model\n",
    "lr_predictions = trained_model.transform(test_linear)\n",
    "evaluator = RegressionEvaluator(labelCol='Appraised_Value_Int', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(lr_predictions)\n",
    "\n",
    "# cast the predictions to integer and show the difference as integer and perentage\n",
    "lr_predictions = lr_predictions.withColumn('prediction_int', lr_predictions['prediction'].cast('int'))\n",
    "lr_predictions = lr_predictions.drop('prediction')\n",
    "lr_predictions = lr_predictions.withColumn('diff', col('Appraised_Value_Int') - col('prediction_int'))\n",
    "lr_predictions = lr_predictions.withColumn('diff_percent', percent_diff_udf(col('prediction_int'), col('Appraised_Value_Int')))\n",
    "\n",
    "# create the view with summary data\n",
    "lr_predictions.select('Account_Num', 'prediction_int', 'diff', 'diff_percent').createOrReplaceTempView(\"linear\")\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE):\", round(rmse, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at some of the data\n",
    "query = \"\"\"\n",
    "select prediction_int, diff, diff_percent, data.*\n",
    "from linear \n",
    "join data on data.Account_Num = linear.Account_Num\n",
    "where data.RP = 'R' \n",
    "  and Appraised_Value_Int > 1000\n",
    "  and Property_Class != 'D1' -- exclude Res-Ag properties\n",
    "order by diff_percent\n",
    "limit 200\n",
    "\"\"\"\n",
    "spark.sql(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
